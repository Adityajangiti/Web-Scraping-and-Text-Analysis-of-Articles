{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XDdmCSxl9Fu",
        "outputId": "766bea4b-dde8-4389-a3f1-25000ab29867"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egb4hMe0oRnC",
        "outputId": "4bdfb32c-2c5c-41bd-ff61-7e1c32969add"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.9/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.9/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy6UNwDZoWwJ",
        "outputId": "4b7d4f11-6837-4c87-f811-1725ae44f246"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.9/dist-packages (0.5.6)\n",
            "Requirement already satisfied: repoze.lru in /usr/local/lib/python3.9/dist-packages (from textstat) (0.7)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.9/dist-packages (from textstat) (0.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlvO3auGoh-Y",
        "outputId": "0839f492-ec61-43f3-8c2a-9ac374dced76"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Read the input.xlsx file\n",
        "df = pd.read_excel('Input.xlsx')\n",
        "\n",
        "# Loop through the URLs and extract the article text\n",
        "for i, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    article = soup.find('article')\n",
        "    article_text = ''\n",
        "    if article:\n",
        "        # Extract the article title\n",
        "        title = article.find('h1').get_text()\n",
        "        article_text += title + '\\n\\n'\n",
        "        # Extract the article text\n",
        "        paragraphs = article.find_all('p')\n",
        "        for p in paragraphs:\n",
        "            article_text += p.get_text() + '\\n\\n'\n",
        "        # Save the article text to a file\n",
        "        with open(f'{url_id}.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(article_text)\n"
      ],
      "metadata": {
        "id": "uKnW3mc3qCGT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the input.xlsx file\n",
        "df = pd.read_excel('output.xlsx')\n",
        "\n",
        "# Loop through the rows and extract the required information\n",
        "output = ''\n",
        "for i, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    positive_score = row['POSITIVE SCORE']\n",
        "    negative_score = row['NEGATIVE SCORE']\n",
        "    polarity_score = row['POLARITY SCORE']\n",
        "    subjectivity_score = row['SUBJECTIVITY SCORE']\n",
        "    avg_sentence_length = row['AVG SENTENCE LENGTH']\n",
        "    percentage_complex_words = row['PERCENTAGE OF COMPLEX WORDS']\n",
        "    fog_index = row['FOG INDEX']\n",
        "    avg_words_per_sentence = row['AVG NUMBER OF WORDS PER SENTENCE']\n",
        "    complex_word_count = row['COMPLEX WORD COUNT']\n",
        "    word_count = row['WORD COUNT']\n",
        "    syllable_per_word = row['SYLLABLE PER WORD']\n",
        "    personal_pronouns = row['PERSONAL PRONOUNS']\n",
        "    avg_word_length = row['AVG WORD LENGTH']\n",
        "    # Format the information as a string\n",
        "    output += f'URL_ID: {url_id}\\nURL: {url}\\nPositive Score: {positive_score}\\nNegative Score: {negative_score}\\n'\n",
        "    output += f'Polarity Score: {polarity_score}\\nSubjectivity Score: {subjectivity_score}\\n'\n",
        "    output += f'Avg Sentence Length: {avg_sentence_length}\\nPercentage of Complex Words: {percentage_complex_words}\\n'\n",
        "    output += f'FOG Index: {fog_index}\\nAvg Number of Words per Sentence: {avg_words_per_sentence}\\n'\n",
        "    output += f'Complex Word Count: {complex_word_count}\\nWord Count: {word_count}\\n'\n",
        "    output += f'Syllable per Word: {syllable_per_word}\\nPersonal Pronouns: {personal_pronouns}\\n'\n",
        "    output += f'Avg Word Length: {avg_word_length}\\n\\n'\n",
        "\n",
        "# Write the output to a text file\n",
        "with open('output.txt', 'w') as f:\n",
        "    f.write(output)\n"
      ],
      "metadata": {
        "id": "MW1dIKQorBHd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-NMJlrLv05b",
        "outputId": "7903a84b-fbd6-4c63-9aa2-6f8b11f14f27"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyoNR4Ke1xdG",
        "outputId": "4587dff8-12ea-4400-9520-aee8752b26f8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import newspaper\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Read the input.xlsx file\n",
        "df = pd.read_excel('Input.xlsx')\n",
        "\n",
        "# Define a function to extract article text and calculate sentiment scores\n",
        "def analyze_article(url):\n",
        "    try:\n",
        "        # Download and parse the article\n",
        "        article = newspaper.Article(url, language='en')\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        \n",
        "        # Extract the article title and text\n",
        "        title = article.title\n",
        "        text = article.text\n",
        "        \n",
        "        # Calculate sentiment scores using TextBlob\n",
        "        blob = TextBlob(text)\n",
        "        positive_score = blob.sentiment.polarity if blob.sentiment.polarity > 0 else 0\n",
        "        negative_score = -blob.sentiment.polarity if blob.sentiment.polarity < 0 else 0\n",
        "        polarity_score = blob.sentiment.polarity\n",
        "        subjectivity_score = blob.sentiment.subjectivity\n",
        "        \n",
        "        # Calculate other metrics\n",
        "        sentences = blob.sentences\n",
        "        avg_sentence_length = len(text) / len(sentences)\n",
        "        words = blob.words\n",
        "        word_count = len(words)\n",
        "        complex_words = [word for word in words if len(word) > 2 and len(TextBlob(word).words) > 2]\n",
        "        complex_word_count = len(complex_words)\n",
        "        percentage_complex_words = 100 * complex_word_count / word_count\n",
        "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "        avg_words_per_sentence = word_count / len(sentences)\n",
        "        \n",
        "        # Catch TextBlob error and return default values\n",
        "        try:\n",
        "            syllable_per_word = sum([len(TextBlob(word).syllables) for word in words]) / word_count\n",
        "        except:\n",
        "            syllable_per_word = 0\n",
        "        \n",
        "        personal_pronouns = sum([1 for word in words if TextBlob(word).tags[0][1] == 'PRP']) / word_count\n",
        "        avg_word_length = sum([len(word) for word in words]) / word_count\n",
        "        \n",
        "        # Return the results as a dictionary\n",
        "        return {\n",
        "            'URL_ID': url.split('/')[-2],\n",
        "            'URL': url,\n",
        "            'POSITIVE SCORE': positive_score,\n",
        "            'NEGATIVE SCORE': negative_score,\n",
        "            'POLARITY SCORE': polarity_score,\n",
        "            'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "            'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "            'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "            'FOG INDEX': fog_index,\n",
        "            'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
        "            'COMPLEX WORD COUNT': complex_word_count,\n",
        "            'WORD COUNT': word_count,\n",
        "            'SYLLABLE PER WORD': syllable_per_word,\n",
        "            'PERSONAL PRONOUNS': personal_pronouns,\n",
        "            'AVG WORD LENGTH': avg_word_length,\n",
        "        }\n",
        "    except:\n",
        "        # Return None if there was an error downloading or parsing the article\n",
        "        return None\n",
        "\n",
        "\n",
        "# Loop through the URLs and analyze each article\n",
        "results = []\n",
        "for url in df['URL']:\n",
        "    result = analyze_article(url)\n",
        "    if result is not None:\n",
        "        results.append(result)\n",
        "\n",
        "# Convert the results to a DataFrame and save to output.xlsx\n",
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_excel('output.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "2juzYlTKsoSW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3J15EZ9txQ2",
        "outputId": "2521ce64-1691-4078-a73a-42f2683e899a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (4.9.2)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (4.6.3)\n",
            "Collecting jieba3k>=0.35.1\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (3.7)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (8.4.0)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting feedparser>=5.2.1\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (6.0)\n",
            "Collecting tldextract>=2.0.1\n",
            "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from newspaper3k) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.1->newspaper3k) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.1->newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.6.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.10.0->newspaper3k) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.10.0->newspaper3k) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.10.0->newspaper3k) (2022.12.7)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.9/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.9.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13552 sha256=854eb8392465d5aedf7029d942a9e36f8b18078422fda995235cc0bebff80f6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/ad/df/a2a01300cea47d5695f242f7e925a805970106fd9e4b151468\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3354 sha256=7108ce0f8566130fa329b6bba5bd5845b1b8b3ca94e8ad2758406c38bec8bd3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/4a/c2/61a371b2524ac90805391c660d8dc4505705297f25e2b85a5d\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398405 sha256=ec45ad2af073e9b90886ad6ce2bb756a1cf45874b382f70df287b9a3ebd38ff7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/22/59/8214a8d6357e9f540ce1f37f9a4362b6156b4ca81b37f1945f\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=6b3d86f41b52e818044c808b78af49806b99e3009d9accd603dcb06d0d651d87\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/7a/a7/78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.4.0\n"
          ]
        }
      ]
    }
  ]
}